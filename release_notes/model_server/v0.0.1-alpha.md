# Model Server v0.0.1-alpha

## Overview

Initial alpha release of the Project Xylen model server. This release provides ML inference as a microservice with FastAPI, supporting LightGBM models optimized with ONNX.

## Features

### Core Functionality

- FastAPI REST API for predictions
- LightGBM model loading and inference
- ONNX optimization for 2-3x faster inference
- Health monitoring with system metrics
- Feedback endpoint for trade outcomes
- Continuous learning data collection

### Endpoints

- POST /predict: Make trading prediction
- GET /health: System and model health metrics
- POST /feedback: Receive trade outcomes
- POST /retrain: Trigger model retraining
- GET /metrics: Prometheus metrics export

### Performance

- Inference latency: 5-15ms (with ONNX)
- Memory footprint: 100-200MB per server
- CPU usage: <10% idle, <30% under load
- Supports concurrent requests

### Model Support

- LightGBM (primary, recommended)
- XGBoost (experimental)
- CatBoost (experimental)
- ONNX format for all above

## Configuration

### Environment Variables

- MODEL_NAME: Server identifier (default: model_1)
- MODEL_PATH: Path to model files (default: ./models/)
- ONNX_ENABLED: Use ONNX optimization (default: true)
- LOG_LEVEL: Logging verbosity (default: INFO)
- PORT: HTTP server port (default: 8001)

### Model Files

Place trained models in `models/` directory:
- lightgbm_model.txt: LightGBM booster
- lightgbm_model.onnx: ONNX optimized version

## Installation

1. Extract model-server package
2. Place trained model in `models/` directory
3. Run: `docker compose up -d model-server-1`
4. Verify health: `curl http://localhost:8001/health`

## Requirements

- Docker 20.10+
- 256MB RAM minimum per server
- Trained LightGBM model file
- Python 3.10+ (for training scripts)

## Training

### Initial Model Training

Located in `initial_model_training_scripts/`:

```bash
cd initial_model_training_scripts
python train_lightgbm_m4.py
```

Output: `lightgbm_model.txt`

### Continuous Learning

Enable in coordinator `config.yaml`:

```yaml
retraining:
  enabled: true
  send_feedback_to_models: true
```

Models automatically collect trade outcomes and retrain incrementally.

## Breaking Changes

None (initial release)

## Known Issues

- ONNX conversion may fail on some XGBoost models (use LightGBM)
- Continuous training requires minimum 100 samples
- Memory usage can spike during retraining (monitor limits)

## Performance Tuning

### ONNX Optimization

Convert model to ONNX:
```bash
python convert_to_onnx.py
```

Benefits: 2-3x faster inference, lower memory

### Resource Limits

Recommended Docker limits:
- CPU: 0.5 cores
- Memory: 256MB

## Scaling

Deploy multiple instances in docker-compose.yml:

```yaml
services:
  model-server-1:
    ports: ["8001:8001"]
  model-server-2:
    ports: ["8002:8002"]
  model-server-3:
    ports: ["8003:8003"]
  model-server-4:
    ports: ["8004:8004"]
```

Update coordinator config.yaml with all endpoints.

## Metrics

Exposed at /metrics:

- model_predictions_total: Prediction count by action
- model_prediction_latency_seconds: Inference time
- model_prediction_confidence: Confidence histogram
- model_retrains_total: Retrain count
- model_training_samples: Samples collected

## Documentation

- docs/MODEL_SETUP.md: Training and deployment guide
- docs/API.md: Endpoint specifications
- docs/ARCHITECTURE.md: System design

## Upgrade Instructions

Not applicable (initial release)

## Security Notes

- No authentication on API (use internal Docker network)
- Do not expose model servers to public internet
- Model files may contain proprietary trading logic

## Support

Check `docker logs xylen-model-1` for errors. Refer to documentation for troubleshooting.

## Contributors

Internal development team

## License

Proprietary - All rights reserved
