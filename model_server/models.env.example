# Project Xylen Model Server Environment Configuration
# Python 3.10.12 compatible - Production deployment
# Copy this to models.env or .env and customize for your host
# Deploy to: /opt/trading_model/models.env on Ubuntu systems

# ============================================================
# MODEL CONFIGURATION
# ============================================================
MODEL_TYPE=lightgbm              # lightgbm, onnx, sklearn, ensemble
MODEL_PATH=/opt/trading_model/models/trading_model.txt
MODEL_FORMAT=lightgbm_txt        # lightgbm_txt, onnx, pickle

# ONNX Runtime (lower memory footprint)
ONNX_MODEL_PATH=/opt/trading_model/models/trading_model.onnx
ONNX_ENABLED=false
ONNX_PROVIDERS=CPUExecutionProvider  # CPUExecutionProvider, CUDAExecutionProvider
ONNX_FALLBACK_TO_LIGHTGBM=true

# Model versioning
MODEL_VERSION=1.0.0
MODEL_REGISTRY_PATH=/opt/trading_model/models/registry.json

# ============================================================
# SERVER CONFIGURATION
# ============================================================
SERVER_HOST=0.0.0.0
SERVER_PORT=8000
SERVER_WORKERS=1                 # Uvicorn workers (keep at 1 for 8GB hosts)
SERVER_LOG_LEVEL=info
SERVER_RELOAD=false              # Hot reload for development only

# ============================================================
# FEATURE PIPELINE
# ============================================================
FEATURE_COUNT=29                 # Expected number of input features
FEATURE_ENGINEERING_ENABLED=true
FEATURE_VALIDATION_STRICT=true   # Reject requests with missing features

# Feature normalization
FEATURE_SCALER_PATH=/opt/trading_model/models/feature_scaler.pkl
FEATURE_SCALER_TYPE=standard     # standard, minmax, robust, none

# ============================================================
# DATA STORAGE & PATHS
# ============================================================
DATA_PATH=/opt/trading_model/data
FEATURE_STORE_PATH=/opt/trading_model/data/feature_store
TRAINING_DATA_PATH=/opt/trading_model/data/training
FEEDBACK_BUFFER_PATH=/opt/trading_model/data/feedback_buffer.parquet
MODEL_CHECKPOINTS_PATH=/opt/trading_model/models/checkpoints

# ============================================================
# RETRAINING CONFIGURATION
# ============================================================
RETRAINING_ENABLED=true
RETRAINING_METHOD=incremental    # incremental, full, scheduled, disabled

# Incremental learning with River
INCREMENTAL_LEARNING_BACKEND=river  # river, lightgbm_refit, online_sgd
INCREMENTAL_LEARNING_RATE=0.01
MIN_SAMPLES_FOR_RETRAIN=100
MAX_FEEDBACK_BUFFER_SIZE=10000

# Full retraining (scheduled via systemd timer)
FULL_RETRAIN_ENABLED=true
FULL_RETRAIN_SCHEDULE="0 2 * * *"  # 2 AM daily
FULL_RETRAIN_MIN_SAMPLES=1000

# Resource constraints (critical for 8GB hosts)
RETRAIN_MIN_FREE_MEMORY_GB=1.2   # Minimum free RAM required
RETRAIN_MAX_SWAP_PERCENT=50      # Don't retrain if swap > 50%
RETRAIN_TIMEOUT_SECONDS=1800     # 30 minute timeout

# LightGBM training parameters
LGBM_MAX_MEMORY_GB=6.0           # For 8GB hosts (leave 2GB for OS)
LGBM_NUM_THREADS=4               # Adjust based on CPU cores
LGBM_MAX_BIN=255                 # 255 for memory efficiency
LGBM_NUM_LEAVES=31
LGBM_LEARNING_RATE=0.05
LGBM_N_ESTIMATORS=100

# ============================================================
# DATA COLLECTION SERVICE (Optional)
# ============================================================
DATA_COLLECTION_ENABLED=false    # Run data_collector.py as separate service
COLLECTION_INTERVAL=300          # 5 minutes
COLLECTION_SYMBOL=BTCUSDT
COLLECTION_TIMEFRAMES=5m,15m,1h
LOOKBACK_PERIODS=100

# ============================================================
# BINANCE API (for data collector)
# ============================================================
BINANCE_API_KEY=your_testnet_api_key_here
BINANCE_API_SECRET=your_testnet_api_secret_here
BINANCE_TESTNET=true
BINANCE_BASE_URL=https://testnet.binancefuture.com

# ============================================================
# MONITORING & OBSERVABILITY
# ============================================================
PROMETHEUS_ENABLED=true
PROMETHEUS_PORT=9091
PROMETHEUS_PATH=/metrics

# Metrics to expose
EXPOSE_MODEL_METRICS=true
EXPOSE_INFERENCE_LATENCY=true
EXPOSE_MEMORY_USAGE=true
EXPOSE_PREDICTION_DISTRIBUTION=true

# Logging
LOG_PATH=/opt/trading_model/logs/model_server.log
LOG_FORMAT=json                  # json, text
LOG_MAX_BYTES=10485760           # 10 MB
LOG_BACKUP_COUNT=10

# ============================================================
# PERFORMANCE TUNING
# ============================================================
# Inference optimization
BATCH_INFERENCE_ENABLED=false
BATCH_MAX_SIZE=32
BATCH_TIMEOUT_MS=100

# Memory management
PRELOAD_MODEL_AT_STARTUP=true
MODEL_CACHE_ENABLED=true
FEATURE_CACHE_TTL_SECONDS=0      # 0 = disabled (always recompute)

# Threading
ASYNC_INFERENCE=false            # Async inference for high concurrency
INFERENCE_THREAD_POOL_SIZE=2

# ============================================================
# HEALTH CHECKS
# ============================================================
HEALTH_CHECK_ENABLED=true
HEALTH_CHECK_MODEL_LOAD=true
HEALTH_CHECK_MEMORY_THRESHOLD_GB=0.5  # Warn if < 500MB free
HEALTH_CHECK_DISK_THRESHOLD_GB=1.0    # Warn if < 1GB free

# ============================================================
# SECURITY (for external exposure via nginx)
# ============================================================
# Note: Internal HTTP APIs are unauthenticated by design
# Use nginx reverse proxy with basic auth for external access
ENABLE_API_KEY_AUTH=false
API_KEY=your_secret_api_key_here

# Rate limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS_PER_MINUTE=60
RATE_LIMIT_BURST=10
